{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHhqKmBLY2JzzmpWiqYyGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoiervese/DL_Project/blob/main/Transformer_con_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlfIKYXYhRlj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La self attention si basa sulla creazione di tre vettori querie key e value\n",
        "se prendiamo una frase in input composta da 4 parole andremo a creare i 3 vettori querie, key, value per ogni parola.\n",
        "\n",
        "Possiamo moltiplicare querie e key^T per ottenere una matrice degli score.\n",
        "Questa ci dice quanta attenzione dobbiamo mettere su ogni parola in relazione alle altre parole della frase.\n",
        "\n",
        "Ogni elemento della matrice deve in seguito essere diviso per la radice della dimensione del vettore querie ( o key ) ottenendo la matrice scaled scores e poi utilizziamo una softmax per avere un valore di probabilità compreso tra 0 e 1, otteniamo così la matrice di attention.\n",
        "\n",
        "La matrice di attenzione deve essere moltiplicata per il vettore value per ottenere così un vettore di output.\n",
        "\n",
        "L'output passerà per un layer lineare per essere processato.\n",
        "\n",
        "Se abbiamo un multihead con N head allora questo procedimento verrà fatto N volte in parallelo.\n",
        "\n",
        "L'output finale sarà il modo in cui ogni parola è legata ad ogni altra parola.\n",
        "\n",
        "Alla fine l'output del multihead attention layer viene sommato all'ingresso iniziale.\n",
        "\n",
        "L'output di questa somma viene inoltre passato in ingresso ad un linear layer e l'uscita viene nuovamente sommata con l'ingresso come in figura.\n",
        "\n",
        "\n",
        "![testo del link](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "iQlBZFh0FjM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "      super(SelfAttention, self).__init__()\n",
        "      self.embed_size = embed_size\n",
        "      self.heads = heads\n",
        "      self.head_dim = embed_size //heads\n",
        "\n",
        "      assert (self.head_dim * heads == embed_size), \"Embed size deve essere divisibile da heads\"\n",
        "\n",
        "\n",
        "      #la self attention si basa sulla creazione di tre vettori querie key e value\n",
        "      #se prendiamo una frase in input composta da 4 parole andremo a creare i 3 vettori querie key value per ogni parola\n",
        "\n",
        "      #nn.Linear(dimensione input, dimensione output)\n",
        "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "      self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len= values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    #Split embedding into self.heads pieces\n",
        "\n",
        "    #reshape rimodella il tensore di ingresso N nella dimensione espressa dai 3 parametri successivi value_len x self.heads x self.head_dim\n",
        "    values= values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "    keys= keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "    values =self.values(values)\n",
        "    keys =self.keys(keys)\n",
        "    queries =self.queries(queries)\n",
        "\n",
        "    #LA FRECCIA QUI POTREBBE DARE PROBLEMI\n",
        "    #einsum calcola il prodotto, in questa eiga andiamo a calcolare la matrice di score  che chiameremo energy\n",
        "    energy= torch.einsum(\"nqhd,nkhd→nhqk\", [query, keys])\n",
        "    #queries shape: (N, query_len, heads, heads_dim)\n",
        "    #keys shape: (N, key_len, heads, heads_dim)\n",
        "    #energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "    #andiamo a realizzare la matrice \"maschera\" che ci\n",
        "    #permetterà di prendere solo le parole successive a quella presa in ingresso e non quelle precedenti\n",
        "    #se non te lo ricordi riguarda il video di spiegazione dei transformer\n",
        "    if mask is not None:\n",
        "        energy = energy.masked_fill(mask ==0, float(\"-1e20\"))\n",
        "\n",
        "    #l'attention è la formula Attention(Q,K,V)=softmax(Q*K^T/(d_k)^(1/2))V\n",
        "    attention = torch.softmax(energy/ (self.embed_size **(1/2)), dim=3)\n",
        "\n",
        "    #einsum calcola il prodotto\n",
        "    out= torch.einsum(\"nhqk,nlhd→nqhd\", [attention, values]).reshape(\n",
        "        N, query_len, self.heds*self.head_dim\n",
        "    )\n",
        "    #attention shape: (N, heads, query_len_key_len)\n",
        "    #values_shape: (N, value_len, heads, heads_dim)\n",
        "    # (N, query_len, heads, head_dim)\n",
        "\n",
        "    out= self.fc_out(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0PbGSL7lhiLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query, mask):\n",
        "    attention = self.attention(value, key, query, mask)\n",
        "\n",
        "    x = self.dropout(self.norm1(attention + query))\n",
        "    forward = self.feed_forward(x)\n",
        "    out = self.dropout(self.norm2(forward + x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "KBPKRZ5YlbMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      device,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      max_length,\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerBlock(\n",
        "                embed_size,\n",
        "                heads,\n",
        "                dropout = dropout,\n",
        "                forward_expansion = forward_expansion\n",
        "            )\n",
        "        for _ in range(num_layers)]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0 , seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out , mask)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "vjyZ5zJ_oBq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(\n",
        "        embed_size, heads, dropout, forward_expansion\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, value, key, src_mask, trg_mask):\n",
        "     attention = self.attention(x, x, x, trg_mask)\n",
        "     query = self.dropout(self.norm(attention + x))\n",
        "     out = self.transformer_block(value, key, query, src_mask)\n",
        "     return out\n"
      ],
      "metadata": {
        "id": "S4SQbOulrGTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               trg_vocab_size,\n",
        "               embed_size,\n",
        "               num_layers,\n",
        "               heads,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               device,\n",
        "               max_length\n",
        "               ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layer = nn.ModuleList(\n",
        "        [DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    )\n",
        "\n",
        "    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def froward(self, x, enc_out, src_mask, trg_mask):\n",
        "    N, seq_length = x.values_shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    x = self.dropout((self.word_embedding(x)+ self.position_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x= layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "    out = self.fc_out(x)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ67AqQ-sjcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      trg_vocab_size,\n",
        "      src_pad_idx,\n",
        "      trg_pad_idx,\n",
        "      embed_size=256,\n",
        "      num_layers=6,\n",
        "      forward_expansion=4,\n",
        "      heads=8,\n",
        "      dropout=0,\n",
        "      device=\"cuda\",\n",
        "      max_length=100\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder =Encoder(\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.decoder= Decoder(\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src!= self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "    N, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "        N, 1, trg_len, trg_len\n",
        "    )\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b-EMQYWivU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mathematics_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK0g2Y7fubMF",
        "outputId": "d05fdaf5-96d2-4cba-d56b-47a502c7e0dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mathematics_dataset\n",
            "  Downloading mathematics_dataset-1.0.1-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m92.2/93.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from mathematics_dataset) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from mathematics_dataset) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mathematics_dataset) (1.16.0)\n",
            "Requirement already satisfied: sympy>=1.2 in /usr/local/lib/python3.10/dist-packages (from mathematics_dataset) (1.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.2->mathematics_dataset) (1.3.0)\n",
            "Installing collected packages: mathematics_dataset\n",
            "Successfully installed mathematics_dataset-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m mathematics_dataset.generate --filter=linear_1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNy_6pjtuivo",
        "outputId": "95ce4145-b236-4cd5-e2b0-3acd7dac0154"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/generate.py\", line 29, in <module>\n",
            "    from mathematics_dataset.modules import modules\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/modules/modules.py\", line 21, in <module>\n",
            "    from mathematics_dataset.modules import algebra\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/modules/algebra.py\", line 25, in <module>\n",
            "    from mathematics_dataset import example\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/example.py\", line 23, in <module>\n",
            "    from mathematics_dataset.util import composition\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/util/composition.py\", line 28, in <module>\n",
            "    from mathematics_dataset.sample import polynomials\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mathematics_dataset/sample/polynomials.py\", line 33, in <module>\n",
            "    from sympy.solvers.diophantine import base_solution_linear as diophantine_solve_linear_2d\n",
            "ImportError: cannot import name 'base_solution_linear' from 'sympy.solvers.diophantine' (/usr/local/lib/python3.10/dist-packages/sympy/solvers/diophantine/__init__.py)\n"
          ]
        }
      ]
    }
  ]
}
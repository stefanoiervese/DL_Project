{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoiervese/DL_Project/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDAFYDRiQNg"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xr2w9xJ9vPi",
        "outputId": "8a429270-4ab1-4564-bfc2-e6de24cee605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.getcwd()\n",
        "path='/content/drive/MyDrive/mathematics_dataset-v1.0/train-easy/arithmetic__add_or_sub.txt'\n",
        "#os.listdir(path)\n",
        "with open(path, \"r\") as file:\n",
        "    # Leggi il contenuto del file\n",
        "    content = file.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08BsEJoGXgmu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "data_list = [x for x in content.split('\\n')]\n",
        "data_list=data_list[:-1]\n",
        "len_data=len(data_list)\n",
        "quest=[]\n",
        "ans=[]\n",
        "for i in range(len_data):\n",
        "  if(i%2==0):\n",
        "    quest.append(data_list[i])\n",
        "  else:\n",
        "    ans.append(data_list[i])\n",
        "coppie = list(zip(quest,ans))\n",
        "random.shuffle(coppie)\n",
        "quest, ans=zip(*coppie)\n",
        "l=int(len(quest)/3)\n",
        "train_q=quest[:2*l]\n",
        "test_q=quest[2*l:]\n",
        "train_a=ans[:2*l]\n",
        "test_a=ans[2*l:]\n",
        "\n",
        "voc_size=len([sentence.split() for sentence in data_list]) #1333332\n",
        "\n",
        "\n",
        "def crea_vocabolario(frasi):\n",
        "    vocabolario = set()\n",
        "\n",
        "    for frase in frasi:\n",
        "        # Utilizza un'espressione regolare per estrarre parole, numeri e simboli dalla frase\n",
        "        parole = re.findall(r'\\d|\\.|[-=+*/()?]|\\S+|\\?', frase.lower())\n",
        "\n",
        "        # Aggiungi le parole al vocabolario\n",
        "        vocabolario.update(parole)\n",
        "\n",
        "\n",
        "    return list(vocabolario)+['unknown']\n",
        "\n",
        "v=crea_vocabolario(data_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzndWSJKjYLG"
      },
      "source": [
        "# Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcr34WNkgaIc",
        "outputId": "7ef17de3-4523-46b6-aa18-e705cf636232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parole sconosciute: ['What']\n",
            "Token IDs: [38, 0, 33, 2]\n"
          ]
        }
      ],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = re.findall(r'\\d|\\.|[-=+*/()?]|\\S+', text)\n",
        "\n",
        "\n",
        "\n",
        "        token_id = []\n",
        "        unknown_tokens = []\n",
        "\n",
        "        new_tokens = []\n",
        "        for token in tokens:\n",
        "          flag=0\n",
        "          if '.' in token and token != '.':\n",
        "            token=token=token.split('.')[0]\n",
        "            flag=1\n",
        "          if token in self.word_to_id:\n",
        "            token_id.append(self.word_to_id[token])\n",
        "          else:\n",
        "                token_id.append(self.word_to_id['unknown'])\n",
        "                unknown_tokens.append(token)\n",
        "          if(flag==1):\n",
        "            token_id.append(self.word_to_id['.'])\n",
        "\n",
        "\n",
        "        if unknown_tokens:\n",
        "            print(\"Parole sconosciute:\", unknown_tokens)\n",
        "\n",
        "        return token_id\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(v)\n",
        "\n",
        "text = \"What is 3?\"\n",
        "token_ids = tokenizer.tokenize(text)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2qqpAtgiexx"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eksaAYSFjxqJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "qt=[]\n",
        "at=[]\n",
        "for x in train_q:\n",
        "  qt.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "for x in train_a:\n",
        "  at.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "\n",
        "qtest=[]\n",
        "atest=[]\n",
        "for x in test_q:\n",
        "  qtest.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "for x in test_a:\n",
        "  atest.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUIrAXy1pejB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb047c2-1e30-4ea4-9520-bd371b0b84c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.0.7-py3-none-any.whl (724 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.1.0-py3-none-any.whl (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.7.1)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.0.7 torchmetrics-1.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcep7jbFtFw_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "voc_size=100\n",
        "src_data = torch.randint(1, voc_size, (3264, 6))\n",
        "tg_data=torch.randint(1,voc_size,(3264,8))\n",
        "fake_dataset=Dataset(src_data,tg_data)\n",
        "fake_loader=DataLoader(fake_dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "max_length1 = max(len(tensor) for tensor in qt)\n",
        "max_length2 = max(len(tensor) for tensor in at)\n",
        "max_length=max(max_length1,max_length2)\n",
        "qtp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in qt], batch_first=True)\n",
        "atp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in at], batch_first=True)\n",
        "\n",
        "\n",
        "max_length1 = max(len(tensor) for tensor in qtest)\n",
        "max_length2 = max(len(tensor) for tensor in atest)\n",
        "max_length=max(max_length1,max_length2)\n",
        "qtestp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in qtest], batch_first=True)\n",
        "atestp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in atest], batch_first=True)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = Dataset(qtp,atp)\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "'''\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda batch: (\n",
        "    pad_sequence([item[0] for item in batch], batch_first=True),\n",
        "    pad_sequence([item[1] for item in batch], batch_first=True)\n",
        "))'''\n",
        "test_dataset = Dataset(qtestp,atestp)\n",
        "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSUfm1fqiGVp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757,
          "referenced_widgets": [
            "3cf3427156dc4d3db8177baf0c49cefc",
            "152b63702d794162b22fcf949d277b5d",
            "4ec6f427bd9b4c17916f80c57ef95ae8",
            "25d957aaf99f4f97a4776472c1459695",
            "45b3491f1b4746c4a33b6abf9d832016",
            "c1595ef999f94ddc91ceb92db0076db6",
            "fa65bcbec26c43b4a7673c302b06c11d",
            "592e9216e15749669d56f6ac89f94ea6",
            "20ed412ebbce405f8d320913b8841e4b",
            "e959ce7b508e4ad89f428471fca46790",
            "61bd9a66fc5940c69e7f82cf6286f2a2"
          ]
        },
        "id": "hnvuAfhEqLBN",
        "outputId": "87685a26-4dcf-43f6-a6c7-37d0d6ad52ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                | Type               | Params\n",
            "-----------------------------------------------------------\n",
            "0 | src_embedding       | Embedding          | 156   \n",
            "1 | tgt_embedding       | Embedding          | 156   \n",
            "2 | transformer_encoder | TransformerEncoder | 14.4 K\n",
            "3 | transformer_decoder | TransformerDecoder | 15.0 K\n",
            "4 | fc                  | Linear             | 195   \n",
            "5 | dropout             | Dropout            | 0     \n",
            "-----------------------------------------------------------\n",
            "29.9 K    Trainable params\n",
            "0         Non-trainable params\n",
            "29.9 K    Total params\n",
            "0.120     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cf3427156dc4d3db8177baf0c49cefc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-99fb0b93cbb4>\u001b[0m in \u001b[0;36m<cell line: 258>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Modifica il numero di epoche come desiderato\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;31m#model_path='/content/drive/MyDrive/mathematics_dataset-v1.0/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 )\n\u001b[1;32m    738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0m_verify_strategy_supports_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/compile.py\u001b[0m in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;34mf\"`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `{type(model).__qualname__}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DataLoader`"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import pytorch_lightning as pl\n",
        "import torch.optim as optim\n",
        "\n",
        "class MultiHeadAttention(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads,dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.emb_dim // num_heads\n",
        "\n",
        "        # Inizializzazione dei moduli lineari per proiettare Q, K, V e l'output.\n",
        "        self.W_q = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_k = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_v = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_o = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(self, q,k,v, mask=None):\n",
        "        q = self.split_heads(self.W_q(q))\n",
        "        k = self.split_heads(self.W_k(k))\n",
        "        v = self.split_heads(self.W_v(v))\n",
        "\n",
        "        att = self.att_score(q, k, v, mask)\n",
        "        out = self.W_o(self.combine_heads(att))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def att_score(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, head_dim = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.multihead_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        att = self.multihead_attention(x,x,x, mask)\n",
        "\n",
        "        add_nor = self.layer_norm1(x + self.dropout(att))\n",
        "        ff_out = self.feedforward(add_nor)\n",
        "        out = self.layer_norm2(add_nor + self.dropout(ff_out))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention Layer (Auto-Attention)\n",
        "        self.self_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Cross-Attention Layer (Attenzione incrociata con l'encoder)\n",
        "        self.cross_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Feedforward Neural Network Layer\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "\n",
        "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        out1 = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attention(out1, encoder_output, encoder_output, src_mask)\n",
        "        out2 = self.layer_norm2(out1 + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        out3 = self.layer_norm3(out2 + self.dropout(ff_output))\n",
        "        return out3\n",
        "\n",
        "class Decoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, num_heads=2, num_layers=6, d_ff=256, max_seq_length=8, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = self.positional_encoding(max_seq_length=max_length, d_model=d_model)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, num_heads, d_ff, dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, num_heads, d_ff, dropout),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_embedded = self.dropout(self.src_embedding(src))\n",
        "        tgt_embedded = self.dropout(self.tgt_embedding(tgt))\n",
        "\n",
        "\n",
        "        src_mask = self.generate_padding_mask(src)\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "        src_mask=None\n",
        "        tgt_mask=None\n",
        "\n",
        "\n",
        "        enc_output = self.transformer_encoder(src_embedded, src_key_padding_mask=src_mask)\n",
        "        dec_output = self.transformer_decoder(tgt_embedded, enc_output, tgt_mask=tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n",
        "\n",
        "    def generate_padding_mask(self, src):\n",
        "        src_mask = (src == 0)\n",
        "        return src_mask\n",
        "\n",
        "    def generate_square_subsequent_mask(self, size):\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 0).transpose(0, 1)\n",
        "        return mask.float()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt)\n",
        "        tgt_mask = (tgt != 0)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt)\n",
        "        tgt_mask = (tgt != 0)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        self.log('val_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "      src, tgt = batch\n",
        "      output = self(src, tgt)\n",
        "      tgt_mask = (tgt != 0)\n",
        "      loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "      self.log('test_loss', loss)  # Registra la loss del test\n",
        "      return loss\n",
        "\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        self.log('test_loss', avg_loss)\n",
        "\n",
        "    def positional_encoding(self, max_seq_length, d_model):\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        positional_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
        "        return positional_encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "voc_len=len(v)\n",
        "t=Transformer(voc_len,voc_len)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "t.to(device)\n",
        "print(device)\n",
        "# Addestra il modello\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=5)  # Modifica il numero di epoche come desiderato\n",
        "trainer.fit(t, train_loader)\n",
        "trainer.test(test_loader)\n",
        "\n",
        "#model_path='/content/drive/MyDrive/mathematics_dataset-v1.0/model'\n",
        "\n",
        "\n",
        "#torch.save(t.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adVxFxb2ydm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "1995775e-0a09-4e36-dfc8-eba8c0687ccb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-69ab31cc5207>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/model'"
          ]
        }
      ],
      "source": [
        "model_path='/content/drive/MyDrive/model'\n",
        "t.load_state_dict(torch.load(model_path))\n",
        "t.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpG2sQjR5SlI"
      },
      "outputs": [],
      "source": [
        "trainer.test(t,dataloaders=test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhkc6Nj_4Ixj"
      },
      "outputs": [],
      "source": [
        "net=nn.Linear(3,4)\n",
        "a=torch.tensor([1.,2.,3.])\n",
        "a=net(a)\n",
        "print(a,torch.argmax(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RaXOrj_8KHy"
      },
      "source": [
        "# TP-Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVK99j1M8OMt"
      },
      "outputs": [],
      "source": [
        "class TPMultiHeadAttention(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads,dropout=0.1):\n",
        "        super(TPMultiHeadAttention, self).__init__()\n",
        "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.emb_dim // num_heads\n",
        "\n",
        "        # Inizializzazione dei moduli lineari per proiettare Q, K, V e l'output.\n",
        "        self.W_q = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_k = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_v = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_r = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_o = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(self, q,k,v,r, mask=None):\n",
        "        q = self.split_heads(self.W_q(q))\n",
        "        k = self.split_heads(self.W_k(k))\n",
        "        v = self.split_heads(self.W_v(v))\n",
        "        r = self.split_heads(self.W_r(r))\n",
        "\n",
        "        att = self.att_score(q, k, v, mask)\n",
        "        filler = torch.tensordot(att, r, dims=0)\n",
        "        diagonal = torch.diagonal(filler)\n",
        "\n",
        "\n",
        "        out = self.W_o(self.combine_heads(att))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def att_score(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, head_dim = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TPEncoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPEncoderLayer, self).__init__()\n",
        "\n",
        "        self.multihead_attention = TPMultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        att = self.multihead_attention(x,x,x,x, mask)\n",
        "\n",
        "        add_nor = self.layer_norm1(x + self.dropout(att))\n",
        "        ff_out = self.feedforward(add_nor)\n",
        "        out = self.layer_norm2(add_nor + self.dropout(ff_out))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TPDecoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPDecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention Layer (Auto-Attention)\n",
        "        self.self_attention = TPMultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Cross-Attention Layer (Attenzione incrociata con l'encoder)\n",
        "        self.cross_attention = TPMultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Feedforward Neural Network Layer\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "\n",
        "        attn_output = self.self_attention(x, x, x,x, tgt_mask)\n",
        "        out1 = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attention(out1, encoder_output, encoder_output, encoder_output, src_mask)\n",
        "        out2 = self.layer_norm2(out1 + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        out3 = self.layer_norm3(out2 + self.dropout(ff_output))\n",
        "        return out3\n",
        "\n",
        "\n",
        "voc_len=len(vocab)\n",
        "t2=Transformer(voc_len,voc_len)\n",
        "trainer2 = pl.Trainer(max_epochs=1)  # Modifica il numero di epoche come desiderato\n",
        "trainer2.fit(t2, train_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHAbGnKgLcAhIdrS9pQYu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3cf3427156dc4d3db8177baf0c49cefc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_152b63702d794162b22fcf949d277b5d",
              "IPY_MODEL_4ec6f427bd9b4c17916f80c57ef95ae8",
              "IPY_MODEL_25d957aaf99f4f97a4776472c1459695"
            ],
            "layout": "IPY_MODEL_45b3491f1b4746c4a33b6abf9d832016"
          }
        },
        "152b63702d794162b22fcf949d277b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1595ef999f94ddc91ceb92db0076db6",
            "placeholder": "​",
            "style": "IPY_MODEL_fa65bcbec26c43b4a7673c302b06c11d",
            "value": "Epoch 0:  11%"
          }
        },
        "4ec6f427bd9b4c17916f80c57ef95ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_592e9216e15749669d56f6ac89f94ea6",
            "max": 6945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20ed412ebbce405f8d320913b8841e4b",
            "value": 780
          }
        },
        "25d957aaf99f4f97a4776472c1459695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e959ce7b508e4ad89f428471fca46790",
            "placeholder": "​",
            "style": "IPY_MODEL_61bd9a66fc5940c69e7f82cf6286f2a2",
            "value": " 780/6945 [00:43&lt;05:46, 17.77it/s, v_num=3]"
          }
        },
        "45b3491f1b4746c4a33b6abf9d832016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c1595ef999f94ddc91ceb92db0076db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa65bcbec26c43b4a7673c302b06c11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "592e9216e15749669d56f6ac89f94ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ed412ebbce405f8d320913b8841e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e959ce7b508e4ad89f428471fca46790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61bd9a66fc5940c69e7f82cf6286f2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
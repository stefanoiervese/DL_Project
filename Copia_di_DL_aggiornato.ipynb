{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoiervese/DL_Project/blob/main/Copia_di_DL_aggiornato.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDAFYDRiQNg"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xr2w9xJ9vPi",
        "outputId": "c075ce74-1bb4-4083-f76c-1f7e5c87c41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['arithmetic__add_or_sub_big.txt', 'arithmetic__div_big.txt', 'algebra__polynomial_roots_big.txt', 'arithmetic__add_sub_multiple_longer.txt', 'arithmetic__mul_div_multiple_longer.txt', 'comparison__kth_biggest_more.txt', 'arithmetic__mixed_longer.txt', 'arithmetic__mul_big.txt', 'comparison__closest_more.txt', 'comparison__sort_more.txt', 'numbers__place_value_big.txt', 'measurement__conversion.txt', 'numbers__round_number_big.txt', 'probability__swr_p_level_set_more_samples.txt', 'probability__swr_p_sequence_more_samples.txt']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.getcwd()\n",
        "path='/content/drive/MyDrive/mathematics_dataset-v1.0/extrapolate/'\n",
        "data_list=[]\n",
        "print(os.listdir(path))\n",
        "for file in os.listdir(path):\n",
        "\n",
        "  with open(path+file, \"r\") as file:\n",
        "    # Leggi il contenuto del file\n",
        "    content = file.read()\n",
        "    data_list=data_list+[x for x in content.split('\\n')]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while '' in data_list:\n",
        "    data_list.remove('')\n"
      ],
      "metadata": {
        "id": "oRHsu2rpKCNw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "08BsEJoGXgmu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "#data_list = [x for x in content.split('\\n')]\n",
        "#data_list=data_list[:-1]\n",
        "len_data=len(data_list)\n",
        "quest=[]\n",
        "ans=[]\n",
        "for i in range(len_data):\n",
        "  if(i%2==0):\n",
        "    quest.append(data_list[i])\n",
        "  else:\n",
        "    data=\"# \" + data_list[i] + \" @\"\n",
        "    ans.append(data)\n",
        "coppie = list(zip(quest,ans))\n",
        "random.shuffle(coppie)\n",
        "quest, ans=zip(*coppie)\n",
        "l=int(len(quest)/3)\n",
        "train_q=quest[:2*l]\n",
        "test_q=quest[2*l:]\n",
        "train_a=ans[:2*l]\n",
        "test_a=ans[2*l:]\n",
        "\n",
        "\n",
        "\n",
        "def crea_vocabolario(frasi):\n",
        "    vocabolario = set()\n",
        "\n",
        "    for frase in frasi:\n",
        "        # Utilizza un'espressione regolare per estrarre parole, numeri e simboli dalla frase\n",
        "        parole = re.findall(r'\\b\\w+\\b|\\d+|[^\\w\\s]', frase.lower())\n",
        "\n",
        "        # Aggiungi le parole al vocabolario\n",
        "        vocabolario.update(parole)\n",
        "\n",
        "\n",
        "    return ['&','#','@','unknown']+list(vocabolario)\n",
        "\n",
        "v=crea_vocabolario(data_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzndWSJKjYLG"
      },
      "source": [
        "# Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Dcr34WNkgaIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f171bc3-8175-4452-ba39-305d348baed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parole sconosciute: ['[', 'ai', ']']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 2, 34517, 3, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = re.findall(r'\\b\\w+\\b|\\d+|[^\\w\\s]', text)\n",
        "\n",
        "\n",
        "\n",
        "        token_id = []\n",
        "        unknown_tokens = []\n",
        "\n",
        "        new_tokens = []\n",
        "        for token in tokens:\n",
        "          flag=0\n",
        "          if '.' in token and token != '.':\n",
        "            token=token=token.split('.')[0]\n",
        "            flag=1\n",
        "          if token in self.word_to_id:\n",
        "            token_id.append(self.word_to_id[token])\n",
        "          else:\n",
        "                token_id.append(self.word_to_id['unknown'])\n",
        "                unknown_tokens.append(token)\n",
        "          if(flag==1):\n",
        "            token_id.append(self.word_to_id['.'])\n",
        "\n",
        "\n",
        "        if unknown_tokens:\n",
        "\n",
        "          print(\"Parole sconosciute:\", unknown_tokens)\n",
        "\n",
        "        return token_id\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(v)\n",
        "\n",
        "tokenizer.tokenize('[@ ?ai] ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2qqpAtgiexx"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "eksaAYSFjxqJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "qt=[]\n",
        "at=[]\n",
        "for x in train_q:\n",
        "  qt.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "for x in train_a:\n",
        "  at.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "\n",
        "qtest=[]\n",
        "atest=[]\n",
        "for x in test_q:\n",
        "  qtest.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "for x in test_a:\n",
        "  atest.append(torch.tensor(tokenizer.tokenize(x.lower())))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ZUIrAXy1pejB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573cad6d-d294-4410-d230-6fd0fc95b7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.0.8-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.7.1)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.0.8 torchmetrics-1.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "tcep7jbFtFw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a4ebf3-a594-4820-e854-704edb959d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#voc_size=100\n",
        "#src_data = torch.randint(1, voc_size, (3264, 6))\n",
        "#tg_data=torch.randint(1,voc_size,(3264,8))\n",
        "#fake_dataset=Dataset(src_data,tg_data)\n",
        "#fake_loader=DataLoader(fake_dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "max_length1 = max(len(tensor) for tensor in qt)\n",
        "max_length2 = max(len(tensor) for tensor in at)\n",
        "max_length=max(max_length1,max_length2)\n",
        "print(max_length)\n",
        "qtp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in qt], batch_first=True)\n",
        "atp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in at], batch_first=True)\n",
        "\n",
        "\n",
        "max_length1 = max(len(tensor) for tensor in qtest)\n",
        "max_length2 = max(len(tensor) for tensor in atest)\n",
        "max_length=max(max_length1,max_length2)\n",
        "qtestp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in qtest], batch_first=True)\n",
        "atestp = pad_sequence([torch.cat([tensor, torch.zeros(max_length - len(tensor),dtype=torch.int)]) for tensor in atest], batch_first=True)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = Dataset(qtp,atp)\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "'''\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda batch: (\n",
        "    pad_sequence([item[0] for item in batch], batch_first=True),\n",
        "    pad_sequence([item[1] for item in batch], batch_first=True)\n",
        "))'''\n",
        "test_dataset = Dataset(qtestp,atestp)\n",
        "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q,a=next(iter(train_loader))\n",
        "q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F8UbC9jJmP5",
        "outputId": "647d383a-7f73-4065-b980-3714a5440194"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def paper_accuracy(predicted_answers, correct_answers):\n",
        "  num_correct_answers = 0\n",
        "  end_of_line=2\n",
        "\n",
        "  for i in range(len(predicted_answers)):\n",
        "\n",
        "    # Preparing predicted answer\n",
        "    single_predicted_answer = torch.argmax(predicted_answers[i], 1).tolist()  # vector of shape (answer_max_length) (concatenates the max value for each row)\n",
        "\n",
        "\n",
        "    single_predicted_answer = single_predicted_answer[1:single_predicted_answer.index(2)]  # removing start and end of line char and additional characters\n",
        "\n",
        "    # Preparing correct answer\n",
        "    single_correct_answer = correct_answers[i].tolist()\n",
        "    single_correct_answer = single_correct_answer[1:single_correct_answer.index(2)]  # removing start of line, end of line and following characters\n",
        "\n",
        "    # If the predicted answer and the correct one have the same exact characters, the predicted answer is correct\n",
        "    #print(\"Predicted\")\n",
        "    #print(single_predicted_answer)\n",
        "    #print('Correct')\n",
        "    #print(single_correct_answer)\n",
        "\n",
        "    if (single_predicted_answer == single_correct_answer):\n",
        "      num_correct_answers += 1\n",
        "\n",
        "  return num_correct_answers/len(predicted_answers)"
      ],
      "metadata": {
        "id": "Fxxbc8jAEZHm"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSUfm1fqiGVp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "hnvuAfhEqLBN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828,
          "referenced_widgets": [
            "a7d8cd00b4454b58aef68a322b4e142e",
            "b955d13a8dac4fe9a8ab225c41893541",
            "0e85c6af15684852a6c5ff2a7e6ec3df",
            "1c8059dd4f904abfbdb8800edf947a68",
            "ac1be0cdc2564675a711f6989d9aea08",
            "67f7bd54b56342e69fdcb6d6766c7d4e",
            "fb54dd62c50749079041cd24ae824ef6",
            "f975687209564d9cb2b33b5c30563bdc",
            "d74e8a5f2faa4ab8ac5ab85ab465991e",
            "afcb628089744823ae21d52e7467e231",
            "2f467191856443dea9ee0754b16079f6"
          ]
        },
        "outputId": "d86a4391-d6a1-4e66-e3e3-8259b4530714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "  rank_zero_warn(\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                | Type      | Params\n",
            "--------------------------------------------------\n",
            "0 | src_embedding       | Embedding | 743 K \n",
            "1 | tgt_embedding       | Embedding | 743 K \n",
            "2 | transformer_encoder | Encoder   | 2.3 K \n",
            "3 | transformer_decoder | Decoder   | 2.9 K \n",
            "4 | fc                  | Linear    | 929 K \n",
            "5 | dropout             | Dropout   | 0     \n",
            "--------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.683     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7d8cd00b4454b58aef68a322b4e142e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-f52d41bfcd9b>\u001b[0m in \u001b[0;36m<cell line: 270>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Modifica il numero di epoche come desiderato\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0m_verify_strategy_supports_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         )\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_idx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;34m\"optimizer_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1268\u001b[0m                         \u001b[0mpg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model_and_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     def _clip_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mconsistent\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPrecisionPlugin\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubclasses\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdirectly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-f52d41bfcd9b>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-f52d41bfcd9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0msrc_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtgt_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import pytorch_lightning as pl\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads,dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.emb_dim // num_heads\n",
        "\n",
        "        # Inizializzazione dei moduli lineari per proiettare Q, K, V e l'output.\n",
        "        self.W_q = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_k = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_v = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_o = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(self, q,k,v):\n",
        "        q = self.split_heads(self.W_q(q))  #64,2,22,2\n",
        "        k = self.split_heads(self.W_k(k))  #64,2,2,22\n",
        "        v = self.split_heads(self.W_v(v))\n",
        "\n",
        "\n",
        "        att = self.att_score(q, k, v)  #62,2,22,22\n",
        "        out = self.W_o(self.combine_heads(att))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def att_score(self, q, k, v):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        attn_scores=self.apply_mask(attn_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, head_dim = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.emb_dim)\n",
        "\n",
        "    def apply_mask(self,tensor):\n",
        "      for i in range(tensor.size(0)):\n",
        "        for j in range(tensor.size(1)):\n",
        "          matrix = tensor[i, j]\n",
        "\n",
        "          mask = torch.triu(torch.ones(matrix.size(0), matrix.size(1)), diagonal=0,).t().to('cuda:0')\n",
        "\n",
        "          matrix = matrix * mask+ (-1e9) * (1 - mask)\n",
        "          tensor[i, j] = matrix\n",
        "          return tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.multihead_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        att = self.multihead_attention(x,x,x)\n",
        "\n",
        "        add_nor = self.layer_norm1(x + self.dropout(att))\n",
        "        ff_out = self.feedforward(add_nor)\n",
        "        out = self.layer_norm2(add_nor + self.dropout(ff_out))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention Layer (Auto-Attention)\n",
        "        self.self_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Cross-Attention Layer (Attenzione incrociata con l'encoder)\n",
        "        self.cross_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Feedforward Neural Network Layer\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "\n",
        "        attn_output = self.self_attention(x, x, x)\n",
        "        out1 = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attention(out1, encoder_output, encoder_output)\n",
        "        out2 = self.layer_norm2(out1 + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        out3 = self.layer_norm3(out2 + self.dropout(ff_output))\n",
        "        return out3\n",
        "\n",
        "class Decoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output)\n",
        "        return x\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, num_heads=2, num_layers=6, d_ff=256, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model=d_model\n",
        "\n",
        "        self.num_heads=num_heads\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.transformer_encoder = Encoder(self.d_model,self.num_heads\n",
        "\n",
        "        )\n",
        "        self.transformer_decoder =Decoder(self.d_model,self.num_heads\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        src,tgt = batch\n",
        "        max_seq_length=src.shape[1]\n",
        "\n",
        "        src_embedded = self.dropout(self.src_embedding(src))+self.positional_encoding(max_seq_length,self.d_model).to('cuda:0')\n",
        "        tgt_embedded = self.dropout(self.tgt_embedding(tgt))+self.positional_encoding(max_seq_length,self.d_model).to('cuda:0')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        enc_output = self.transformer_encoder(src_embedded)\n",
        "        dec_output = self.transformer_decoder(tgt_embedded, enc_output)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(batch)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        self.log('val_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        batch_q, batch_a = batch\n",
        "\n",
        "      # Computing prediction and accuracy\n",
        "        pred = self(batch)  # shape (batch_size, answer_max_length, dict_size)\n",
        "        accuracy = paper_accuracy(pred, batch_a)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "\n",
        "        #print_correct(self.dictionary, batch, pred)\n",
        "\n",
        "        #print(accuracy)  # at the end of every epoch it is logged the average of the accuracies of each batch\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def positional_encoding(self, max_seq_length, d_model):\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        positional_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
        "        return positional_encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc=[]\n",
        "losses=[]\n",
        "voc_len=len(v)\n",
        "t=Transformer(voc_len,voc_len)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "t.to(device)\n",
        "print(device)\n",
        "# Addestra il modello\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=3)  # Modifica il numero di epoche come desiderato\n",
        "trainer.fit(t, train_loader)\n",
        "\n",
        "\n",
        "#model_path='/content/drive/MyDrive/mathematics_dataset-v1.0/model'\n",
        "\n",
        "\n",
        "#torch.save(t.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(acc)/len(acc)"
      ],
      "metadata": {
        "id": "faI6lBC0-vk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f4bb30-2312-4480-b2cd-5f12687d1a1b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8277239469787339"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xpG2sQjR5SlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "3a17aa0d9c294e1caae3ae7dafc1e463",
            "5efab40f314942869d983f48a29c53da",
            "cf7ae3fc37854f289fd9e19f09dbde31",
            "d0e97656f33d47a0b48046bdf3c32034",
            "f53ee952819641759b58c7c49da5d42b",
            "8ac8a9c2d895401b81c340291ac44439",
            "b9bc96485d954277be67552ce92920e5",
            "bab3376110354b7dbe0245b843d95cc5",
            "1276c5e9cfc74befa85b0d3298b63a54",
            "981747b63192448a94c5698ce6fe08e2",
            "7928d4fbbef5420d969399ccdbe83ea4"
          ]
        },
        "outputId": "c5f20213-ae7f-4b0a-accd-22d23a31e356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a17aa0d9c294e1caae3ae7dafc1e463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model_path='/content/drive/MyDrive/modellino.pth'\n",
        "#torch.save(t.state_dict(), model_path)\n",
        "trainer.test(t,test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(t,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "bb00bc0ad4e346b2a594199556f3bc3c",
            "e805746f21e34099995d98fcaea9d63d",
            "1b5809b50e2f40239471d6a235596c6a",
            "b77dcc4c55764457bd5c544b3302678b",
            "813c6fcd51fc43a88955e3318e6bfb0f",
            "924dee4a0583426f88d5d2460c5f8c83",
            "7daf80c01803486bb389596573973356",
            "da973c49048745aaa2129d8897b5eea9",
            "ec486ea12ea5424cba036af417efaf5a",
            "cfef0976aafe4bf28db812c5e32a0bba",
            "2fa40ae8c35947e0b0cd407ef0461385"
          ]
        },
        "id": "_4Heqj0VVXF6",
        "outputId": "8eaecdde-f945-4168-f442-7aa454472172"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb00bc0ad4e346b2a594199556f3bc3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-f1fed77ba32b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0m_verify_strategy_supports_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    743\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         )\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0;31m# remove the tensors from the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tensors_to_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mprevious_dataloader_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-4960374722c5>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0;31m# Computing prediction and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch_size, answer_max_length, dict_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaper_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_a\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# accuracy for the current batch as defined in the \"mathematics dataset\" paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m#print_correct(self.dictionary, batch, pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bd22307c2b6f>\u001b[0m in \u001b[0;36mpaper_accuracy\u001b[0;34m(predicted_answers, correct_answers)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msingle_predicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_predicted_answer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_predicted_answer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# removing start and end of line char and additional characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Preparing correct answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 2 is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(acc)/len(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3shG2RuoVpO_",
        "outputId": "3d8bca1f-262f-4d1b-a944-149a5608ab64"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9502231500143967"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g77qbpYn07y4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "5a7a489d-1e15-4966-e6ea-b14da811bf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(t.state_dict(), 'trainato')"
      ],
      "metadata": {
        "id": "JDHGggG4y-A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.load_state_dict(torch.load('/content/drive/MyDrive/modellino.pth'))"
      ],
      "metadata": {
        "id": "MXQI9fjOmLLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a36909-ca79-4be9-f7d1-ce541294cbab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RaXOrj_8KHy"
      },
      "source": [
        "# TP-Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TPMultiHeadAttention(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads,dropout=0.1):\n",
        "        super(TPMultiHeadAttention, self).__init__()\n",
        "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.emb_dim // num_heads\n",
        "\n",
        "        # Inizializzazione dei moduli lineari per proiettare Q, K, V e l'output.\n",
        "        self.W_q = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_k = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_v = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_r = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.W_o = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(self, q,k,v,r):\n",
        "        q = self.split_heads(self.W_q(q))  #64,2,22,2\n",
        "        k = self.split_heads(self.W_k(k))  #64,2,2,22\n",
        "        v = self.split_heads(self.W_v(v))\n",
        "        r = self.split_heads(self.W_r(r))\n",
        "\n",
        "\n",
        "        att = self.att_score(q, k, v)  #62,2,22,22\n",
        "        #filler = torch.tensordot(att, r, dims=0)\n",
        "        #diagonal = torch.diagonal(filler)\n",
        "        #print(att.shape,filler.shape,diagonal.shape)\n",
        "\n",
        "\n",
        "        out = self.W_o(self.combine_heads(att*r))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "    def att_score(self, q, k, v):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        attn_scores=self.apply_mask(attn_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, head_dim = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.emb_dim)\n",
        "\n",
        "    def apply_mask(self,tensor):\n",
        "      for i in range(tensor.size(0)):\n",
        "        for j in range(tensor.size(1)):\n",
        "          matrix = tensor[i, j]\n",
        "\n",
        "          mask = torch.triu(torch.ones(matrix.size(0), matrix.size(1)), diagonal=0,).t().to('cuda:0')\n",
        "\n",
        "          matrix = matrix * mask+ (-1e9) * (1 - mask)\n",
        "          tensor[i, j] = matrix\n",
        "          return tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TPEncoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPEncoderLayer, self).__init__()\n",
        "\n",
        "        self.multihead_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        att = self.multihead_attention(x,x,x,x)\n",
        "\n",
        "        add_nor = self.layer_norm1(x + self.dropout(att))\n",
        "        ff_out = self.feedforward(add_nor)\n",
        "        out = self.layer_norm2(add_nor + self.dropout(ff_out))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TPEncoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPEncoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TPEncoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TPDecoderLayer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPDecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention Layer (Auto-Attention)\n",
        "        self.self_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Cross-Attention Layer (Attenzione incrociata con l'encoder)\n",
        "        self.cross_attention = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "        # Feedforward Neural Network Layer\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(emb_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "\n",
        "        attn_output = self.self_attention(x, x, x,x)\n",
        "        out1 = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attention(out1, encoder_output,encoder_output, encoder_output)\n",
        "        out2 = self.layer_norm2(out1 + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        out3 = self.layer_norm3(out2 + self.dropout(ff_output))\n",
        "        return out3\n",
        "\n",
        "class TPDecoder(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, num_heads, num_layers=6, feedforward_dim=32, dropout=0.1):\n",
        "        super(TPDecoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TPDecoderLayer(emb_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class TPTransformer(pl.LightningModule):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, num_heads=2, num_layers=6, d_ff=256, dropout=0.1):\n",
        "        super(TPTransformer, self).__init__()\n",
        "        self.d_model=d_model\n",
        "\n",
        "        self.num_heads=num_heads\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.transformer_encoder = TPEncoder(self.d_model,self.num_heads\n",
        "\n",
        "        )\n",
        "        self.transformer_decoder =TPDecoder(self.d_model,self.num_heads\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        src,tgt = batch\n",
        "        max_seq_length=src.shape[1]\n",
        "\n",
        "        src_embedded = self.dropout(self.src_embedding(src))+self.positional_encoding(max_seq_length,self.d_model).to('cuda:0')\n",
        "        tgt_embedded = self.dropout(self.tgt_embedding(tgt))+self.positional_encoding(max_seq_length,self.d_model).to('cuda:0')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        enc_output = self.transformer_encoder(src_embedded)\n",
        "        dec_output = self.transformer_decoder(tgt_embedded, enc_output)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(batch)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "        self.log('val_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        batch_q, batch_a = batch\n",
        "\n",
        "      # Computing prediction and accuracy\n",
        "        pred = self(batch)  # shape (batch_size, answer_max_length, dict_size)\n",
        "        accuracy = paper_accuracy(pred, batch_a)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "\n",
        "        #print_correct(self.dictionary, batch, pred)\n",
        "\n",
        "        #print(accuracy)  # at the end of every epoch it is logged the average of the accuracies of each batch\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def positional_encoding(self, max_seq_length, d_model):\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        positional_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
        "        return positional_encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc=[]\n",
        "losses=[]\n",
        "voc_len=len(v)\n",
        "t=Transformer(voc_len,voc_len)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "t.to(device)\n",
        "print(device)\n",
        "# Addestra il modello\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=3)  # Modifica il numero di epoche come desiderato\n",
        "trainer.fit(t, train_loader)\n",
        "\n",
        "\n",
        "#model_path='/content/drive/MyDrive/mathematics_dataset-v1.0/model'\n",
        "\n",
        "\n",
        "#torch.save(t.state_dict(), model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "360939eacdd34c2e90e91bf4cedae8cd",
            "40b9edab3b79465480960ef3064d4cab",
            "0771d54a121e434392366c38bf8bc67c",
            "480b994297d940daa6e62c902c01038d",
            "bc06e121934e47b39fda4fb3d3ff0530",
            "c4b9958c84d9431b8d656f444e6ab152",
            "91e4aa8cf3164ddcb1c69df9e80f037d",
            "1c12b7fcb20e46e794e29ca7931b927d",
            "e548f579aa1b48d298312f0a1ce5f0cb",
            "c3b10e59d7394e9eabaf13eaeca6a62f",
            "77ea0162892845d5b0a757672a7cd9ec"
          ]
        },
        "id": "24TtpczCL-Zk",
        "outputId": "4068be03-3485-421e-961d-448c099b29a8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                | Type      | Params\n",
            "--------------------------------------------------\n",
            "0 | src_embedding       | Embedding | 284   \n",
            "1 | tgt_embedding       | Embedding | 284   \n",
            "2 | transformer_encoder | Encoder   | 2.4 K \n",
            "3 | transformer_decoder | Decoder   | 3.1 K \n",
            "4 | fc                  | Linear    | 355   \n",
            "5 | dropout             | Dropout   | 0     \n",
            "--------------------------------------------------\n",
            "6.5 K     Trainable params\n",
            "0         Non-trainable params\n",
            "6.5 K     Total params\n",
            "0.026     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "360939eacdd34c2e90e91bf4cedae8cd"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7d8cd00b4454b58aef68a322b4e142e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b955d13a8dac4fe9a8ab225c41893541",
              "IPY_MODEL_0e85c6af15684852a6c5ff2a7e6ec3df",
              "IPY_MODEL_1c8059dd4f904abfbdb8800edf947a68"
            ],
            "layout": "IPY_MODEL_ac1be0cdc2564675a711f6989d9aea08"
          }
        },
        "b955d13a8dac4fe9a8ab225c41893541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67f7bd54b56342e69fdcb6d6766c7d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_fb54dd62c50749079041cd24ae824ef6",
            "value": "Epoch 0:   0%"
          }
        },
        "0e85c6af15684852a6c5ff2a7e6ec3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f975687209564d9cb2b33b5c30563bdc",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d74e8a5f2faa4ab8ac5ab85ab465991e",
            "value": 0
          }
        },
        "1c8059dd4f904abfbdb8800edf947a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afcb628089744823ae21d52e7467e231",
            "placeholder": "​",
            "style": "IPY_MODEL_2f467191856443dea9ee0754b16079f6",
            "value": " 0/1563 [00:00&lt;?, ?it/s]"
          }
        },
        "ac1be0cdc2564675a711f6989d9aea08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "67f7bd54b56342e69fdcb6d6766c7d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb54dd62c50749079041cd24ae824ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f975687209564d9cb2b33b5c30563bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d74e8a5f2faa4ab8ac5ab85ab465991e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afcb628089744823ae21d52e7467e231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f467191856443dea9ee0754b16079f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a17aa0d9c294e1caae3ae7dafc1e463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5efab40f314942869d983f48a29c53da",
              "IPY_MODEL_cf7ae3fc37854f289fd9e19f09dbde31",
              "IPY_MODEL_d0e97656f33d47a0b48046bdf3c32034"
            ],
            "layout": "IPY_MODEL_f53ee952819641759b58c7c49da5d42b"
          }
        },
        "5efab40f314942869d983f48a29c53da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ac8a9c2d895401b81c340291ac44439",
            "placeholder": "​",
            "style": "IPY_MODEL_b9bc96485d954277be67552ce92920e5",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "cf7ae3fc37854f289fd9e19f09dbde31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bab3376110354b7dbe0245b843d95cc5",
            "max": 3473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1276c5e9cfc74befa85b0d3298b63a54",
            "value": 3473
          }
        },
        "d0e97656f33d47a0b48046bdf3c32034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_981747b63192448a94c5698ce6fe08e2",
            "placeholder": "​",
            "style": "IPY_MODEL_7928d4fbbef5420d969399ccdbe83ea4",
            "value": " 3473/3473 [01:01&lt;00:00, 56.78it/s]"
          }
        },
        "f53ee952819641759b58c7c49da5d42b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8ac8a9c2d895401b81c340291ac44439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9bc96485d954277be67552ce92920e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bab3376110354b7dbe0245b843d95cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1276c5e9cfc74befa85b0d3298b63a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "981747b63192448a94c5698ce6fe08e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7928d4fbbef5420d969399ccdbe83ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb00bc0ad4e346b2a594199556f3bc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e805746f21e34099995d98fcaea9d63d",
              "IPY_MODEL_1b5809b50e2f40239471d6a235596c6a",
              "IPY_MODEL_b77dcc4c55764457bd5c544b3302678b"
            ],
            "layout": "IPY_MODEL_813c6fcd51fc43a88955e3318e6bfb0f"
          }
        },
        "e805746f21e34099995d98fcaea9d63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_924dee4a0583426f88d5d2460c5f8c83",
            "placeholder": "​",
            "style": "IPY_MODEL_7daf80c01803486bb389596573973356",
            "value": "Testing DataLoader 0:   0%"
          }
        },
        "1b5809b50e2f40239471d6a235596c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da973c49048745aaa2129d8897b5eea9",
            "max": 3473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec486ea12ea5424cba036af417efaf5a",
            "value": 0
          }
        },
        "b77dcc4c55764457bd5c544b3302678b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfef0976aafe4bf28db812c5e32a0bba",
            "placeholder": "​",
            "style": "IPY_MODEL_2fa40ae8c35947e0b0cd407ef0461385",
            "value": " 0/3473 [00:00&lt;?, ?it/s]"
          }
        },
        "813c6fcd51fc43a88955e3318e6bfb0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "924dee4a0583426f88d5d2460c5f8c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7daf80c01803486bb389596573973356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da973c49048745aaa2129d8897b5eea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec486ea12ea5424cba036af417efaf5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfef0976aafe4bf28db812c5e32a0bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa40ae8c35947e0b0cd407ef0461385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "360939eacdd34c2e90e91bf4cedae8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40b9edab3b79465480960ef3064d4cab",
              "IPY_MODEL_0771d54a121e434392366c38bf8bc67c",
              "IPY_MODEL_480b994297d940daa6e62c902c01038d"
            ],
            "layout": "IPY_MODEL_bc06e121934e47b39fda4fb3d3ff0530"
          }
        },
        "40b9edab3b79465480960ef3064d4cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b9958c84d9431b8d656f444e6ab152",
            "placeholder": "​",
            "style": "IPY_MODEL_91e4aa8cf3164ddcb1c69df9e80f037d",
            "value": "Epoch 0:   2%"
          }
        },
        "0771d54a121e434392366c38bf8bc67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c12b7fcb20e46e794e29ca7931b927d",
            "max": 6945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e548f579aa1b48d298312f0a1ce5f0cb",
            "value": 140
          }
        },
        "480b994297d940daa6e62c902c01038d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b10e59d7394e9eabaf13eaeca6a62f",
            "placeholder": "​",
            "style": "IPY_MODEL_77ea0162892845d5b0a757672a7cd9ec",
            "value": " 140/6945 [00:08&lt;06:30, 17.42it/s, v_num=33]"
          }
        },
        "bc06e121934e47b39fda4fb3d3ff0530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c4b9958c84d9431b8d656f444e6ab152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e4aa8cf3164ddcb1c69df9e80f037d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c12b7fcb20e46e794e29ca7931b927d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e548f579aa1b48d298312f0a1ce5f0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3b10e59d7394e9eabaf13eaeca6a62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ea0162892845d5b0a757672a7cd9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}